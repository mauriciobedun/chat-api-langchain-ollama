# ğŸ¤– API de Chat com FastAPI + LangChain + Ollama (Docker)

ğŸ¯ **Objetivo**: API de chat de Perguntas & Respostas usando FastAPI, LangChain e Ollama, totalmente containerizada com Docker.

## ğŸ“‹ PrÃ©-requisitos

- **Docker** e **Docker Compose** instalados
- **4GB+ de RAM livre** (para modelos LLM)
- **ConexÃ£o com internet** (para baixar modelo na primeira execuÃ§Ã£o)

## ğŸš€ Quick Start (2 comandos)

```bash
# 1. Clone o repositÃ³rio
git clone <repo-url>
cd DESAFIO-DGTALLAB

# 2. Iniciar tudo com Docker
docker-compose up -d
```

**ğŸ‰ Pronto!** 
- **API**: http://localhost:8000
- **DocumentaÃ§Ã£o**: http://localhost:8000/docs
- **Health Check**: http://localhost:8000/api/v1/health

## ğŸ“š Comandos Ãšteis
```bash
# Iniciar serviÃ§os
docker-compose up -d

# Ver logs
docker-compose logs -f

# Parar serviÃ§os
docker-compose down

# Reconstruir imagens
docker-compose build --no-cache

# Ver status
docker-compose ps
```

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI API   â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Ollama LLM    â”‚
â”‚   (Port 8000)   â”‚   Requests  â”‚   (Port 11434)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                               â”‚
        â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LangChain +    â”‚             â”‚  llama3 Model   â”‚
â”‚  Vector Store   â”‚             â”‚   (Auto-pull)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“š DocumentaÃ§Ã£o da API

### Endpoints Principais

#### ğŸ—¨ï¸ POST `/api/v1/chat`
Chat bÃ¡sico com o LLM.

**Request:**
```json
{
  "message": "Qual a capital da FranÃ§a?",
  "session_id": "user-123"
}
```

**Response:**
```json
{
  "answer": "A capital da FranÃ§a Ã© Paris.",
  "latency_ms": 1250,
  "session_id": "user-123",
  "timestamp": "2024-01-01T12:00:00Z"
}
```

#### ğŸ“„ POST `/api/v1/upload-document`
Upload de documentos para RAG.

**Request:** (form-data)
- `file`: Arquivo .txt ou .md

**Response:**
```json
{
  "message": "Documento exemplo.txt carregado com sucesso",
  "filename": "exemplo.txt",
  "size": 1234,
  "total_documents": 1
}
```

#### â“ POST `/api/v1/ask`
Perguntas sobre documentos carregados (RAG).

**Request:**
```json
{
  "question": "O que Ã© machine learning?",
  "session_id": "user-123"
}
```

**Response:**
```json
{
  "answer": "Machine Learning Ã© uma subÃ¡rea da IA...",
  "sources": ["[exemplo.txt] Machine Learning Ã© uma subÃ¡rea..."],
  "latency_ms": 2340,
  "session_id": "user-123",
  "timestamp": "2024-01-01T12:00:00Z"
}
```

#### ğŸ¥ GET `/api/v1/health`
Health check dos serviÃ§os.

**Response:**
```json
{
  "status": "healthy",
  "timestamp": "2024-01-01T12:00:00Z",
  "ollama_status": "connected",
  "model": "llama3",
  "documents_loaded": 1,
  "active_sessions": 2
}
```

## ğŸ§ª Testando a API

### Teste Automatizado
```bash
# Executar todos os testes
docker-compose exec api python test_api.py
```

### Testes Manuais com curl
```bash
# Health check
curl http://localhost:8000/api/v1/health

# Chat bÃ¡sico
curl -X POST "http://localhost:8000/api/v1/chat" \
     -H "Content-Type: application/json" \
     -d '{"message": "OlÃ¡! Como vocÃª estÃ¡?"}'

# Upload documento
curl -X POST "http://localhost:8000/api/v1/upload-document" \
     -F "file=@example_document.txt"

# Pergunta sobre documento
curl -X POST "http://localhost:8000/api/v1/ask" \
     -H "Content-Type: application/json" \
     -d '{"question": "O que Ã© inteligÃªncia artificial?"}'
```

### Teste com Python
```python
import requests

# Health check
response = requests.get("http://localhost:8000/api/v1/health")
print(f"Status: {response.json()}")

# Chat
response = requests.post(
    "http://localhost:8000/api/v1/chat",
    json={"message": "Explique machine learning"}
)
print(f"Resposta: {response.json()}")
```

## ğŸ”§ Desenvolvimento

### Modo Desenvolvimento (Hot Reload)
```bash
# Iniciar com hot reload
docker-compose -f docker-compose.yml -f docker-compose.dev.yml up
```

### Acessar Containers
```bash
# Shell da API
docker-compose exec api bash

# Shell do Ollama  
docker-compose exec ollama bash

# Ver logs especÃ­ficos
docker-compose logs -f api
docker-compose logs -f ollama
```

### Estrutura do Projeto
```
â”œâ”€â”€ app/                    # CÃ³digo da aplicaÃ§Ã£o
â”‚   â”œâ”€â”€ main.py            # FastAPI app
â”‚   â”œâ”€â”€ api/               # Endpoints
â”‚   â”œâ”€â”€ services/          # LÃ³gica de negÃ³cio
â”‚   â””â”€â”€ schemas/           # Modelos Pydantic
â”œâ”€â”€ docker-compose.yml     # ConfiguraÃ§Ã£o principal
â”œâ”€â”€ docker-compose.dev.yml # ConfiguraÃ§Ã£o desenvolvimento
â”œâ”€â”€ Dockerfile             # Imagem da API
â”œâ”€â”€ requirements.txt      # DependÃªncias Python
â””â”€â”€ test_api.py           # Testes automatizados
```

## ğŸš€ Recursos Implementados

### âœ… Funcionalidades BÃ¡sicas
- **Chat com LLM**: Conversas usando llama3 via Ollama
- **MediÃ§Ã£o de latÃªncia**: Tempo de resposta em milissegundos  
- **ValidaÃ§Ã£o de dados**: Schemas Pydantic robustos
- **Health checks**: Monitoramento de saÃºde dos serviÃ§os
- **DocumentaÃ§Ã£o automÃ¡tica**: Swagger UI integrado

### âœ… Funcionalidades AvanÃ§adas
- **RAG (Retrieval Augmented Generation)**:
  - Upload de documentos (.txt, .md)
  - Busca semÃ¢ntica com FAISS
  - Respostas baseadas em documentos
- **MemÃ³ria de conversaÃ§Ã£o**:
  - SessÃµes por usuÃ¡rio
  - HistÃ³rico mantido por sessÃ£o
- **Observabilidade**:
  - Logs estruturados
  - MÃ©tricas de performance
  - Headers de tempo de processamento

### âœ… DevOps e ProduÃ§Ã£o
- **ContainerizaÃ§Ã£o completa**: Docker + Docker Compose
- **OrquestraÃ§Ã£o de serviÃ§os**: API + Ollama + InicializaÃ§Ã£o automÃ¡tica
- **Health checks**: VerificaÃ§Ã£o automÃ¡tica de saÃºde
- **Volumes persistentes**: Dados do Ollama mantidos
- **Rede isolada**: ComunicaÃ§Ã£o segura entre containers

## ğŸ“Š Monitoramento

### Verificar Status
```bash
# Status geral
docker-compose ps

# Health check
curl http://localhost:8000/api/v1/health
curl http://localhost:11434/api/tags

# Logs em tempo real
docker-compose logs -f
```

### MÃ©tricas DisponÃ­veis
- LatÃªncia de respostas (ms)
- NÃºmero de sessÃµes ativas
- Documentos carregados
- Status de conexÃ£o Ollama

## ğŸ› Troubleshooting

### ServiÃ§os nÃ£o iniciam
```bash
# Verificar logs
docker-compose logs -f

# Reconstruir imagens
docker-compose build --no-cache
docker-compose up -d
```

### Ollama nÃ£o baixa modelo
```bash
# Verificar logs do Ollama
docker-compose logs -f ollama

# Reiniciar serviÃ§o
docker-compose restart ollama ollama-init
```

### API nÃ£o responde
```bash
# Verificar saÃºde
curl http://localhost:8000/api/v1/health

# Reiniciar API
docker-compose restart api
```

### Limpar tudo e recomeÃ§ar
```bash
docker-compose down -v
docker system prune -f
docker-compose build --no-cache
docker-compose up -d
```

### Problemas de memÃ³ria
- **Requisito**: MÃ­nimo 4GB RAM livre
- **SoluÃ§Ã£o**: Fechar outras aplicaÃ§Ãµes pesadas
- **Alternativa**: Usar modelo menor (ajustar em docker-compose.yml)

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch: `git checkout -b feature/nova-funcionalidade`
3. Commit: `git commit -m 'Adicionar nova funcionalidade'`
4. Push: `git push origin feature/nova-funcionalidade`
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para detalhes.

---

ğŸ‰ **Projeto pronto para avaliaÃ§Ã£o!** 
Basta executar `docker-compose up -d` e testar em http://localhost:8000/docs