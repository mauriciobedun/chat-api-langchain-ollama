Desafio TÃ©cnico â€“ Dev Python (FastAPI + LangChain)
ğŸ¯ Objetivo
Criar uma API de chat de Perguntas & Respostas usando FastAPI e LangChain.

O sistema deve receber uma pergunta do usuÃ¡rio e responder utilizando um LLM conectado via LangChain.

âœ… Requisitos mÃ­nimos
Endpoint /chat

MÃ©todo: POST
Entrada (exemplo):
{"message": "Qual a capital da FranÃ§a?"}
SaÃ­da (exemplo):
{
  "answer": "A capital da FranÃ§a Ã© Paris.",
  "latency_ms": 123
}
Uso de LangChain

Implementar uma chain simples (LLMChain) com um prompt bÃ¡sico.
Retornar a resposta do modelo e medir o tempo de execuÃ§Ã£o.
Estrutura do projeto

CÃ³digo organizado em mÃ³dulos, exemplo:
app/
  â”œâ”€â”€ main.py
  â”œâ”€â”€ api/
  â”‚    â””â”€â”€ chat.py
  â”œâ”€â”€ services/
  â”‚    â””â”€â”€ qa.py
  â””â”€â”€ schemas/
       â””â”€â”€ chat.py
Pydantic para schemas de request/response.
.env para variÃ¡veis de configuraÃ§Ã£o.
ConfiguraÃ§Ã£o

A API deve rodar mesmo sem chave paga de LLM.
Deve estar documentado no README como executar (uvicorn app.main:app --reload).
ğŸ”„ Alternativas para nÃ£o depender de LLM paga
VocÃª pode escolher uma das opÃ§Ãµes abaixo:

Mock LLM (mais simples)

Usar FakeListLLM do LangChain para responder com textos prÃ©-definidos.
from langchain.llms import FakeListLLM
llm = FakeListLLM(responses=["Paris Ã© a capital da FranÃ§a."])
Modelos open-source gratuitos

Hugging Face Inference API (gratuita, precisa sÃ³ de token do Hugging Face).
curl https://api-inference.huggingface.co/models/distilbert-base-uncased \
     -H "Authorization: Bearer hf_xxx" \
     -d '{"inputs": "Hello world"}'
Ollama (executa modelos localmente, ex.: llama3, mistral).
Instale Ollama e rode:
ollama run llama3
ğŸš€ Desafios adicionais (opcionais)
IntegraÃ§Ã£o com RAG (Retrieval Augmented Generation)

Criar um endpoint /ask que permita o upload de documentos .txt ou .md.
Indexar o conteÃºdo em FAISS ou Chroma.
Fazer perguntas sobre esses documentos e retornar trechos citados como fontes.
MemÃ³ria de conversa

Permitir que um session_id mantenha o histÃ³rico do chat.
Observabilidade

Adicionar /health para mostrar status da API.
Log estruturado com tempo de execuÃ§Ã£o por requisiÃ§Ã£o.
ğŸ“¦ Entrega
RepositÃ³rio GitHub (ou ZIP) com:
CÃ³digo fonte.
requirements.txt.
README com instruÃ§Ãµes de instalaÃ§Ã£o, execuÃ§Ã£o e exemplos de requisiÃ§Ãµes.
ğŸ“ CritÃ©rios de avaliaÃ§Ã£o
Funcionalidade bÃ¡sica do chat atendida.
CÃ³digo limpo e organizado.
Uso correto do FastAPI e LangChain.
README claro e objetivo.
(BÃ´nus) Desafios adicionais implementados.