services:
  # Serviço Ollama
  ollama:
    image: ollama/ollama:latest
    container_name: chat-api-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./ollama-entrypoint.sh:/usr/local/bin/ollama-entrypoint.sh:ro
    environment:
      - OLLAMA_ORIGINS=*
      - OLLAMA_MODEL=llama3           # parametriza o modelo
      - OLLAMA_AUTO_PULL=1            # habilita o pull no entrypoint
    entrypoint: ["/bin/sh", "/usr/local/bin/ollama-entrypoint.sh"]
    healthcheck:
      # checa se o servidor está de pé (não exige o modelo pronto)
      test: ["CMD-SHELL", "ollama list >/dev/null 2>&1 || exit 1"]
      interval: 15s
      timeout: 10s
      retries: 40          # ~10 minutos de janela
      start_period: 0s
    networks:
      - chat-network

  # Serviço da API
  api:
    build: .
    container_name: chat-api-fastapi
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_MODEL=llama3
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - ./app:/app/app  # Para desenvolvimento com hot reload
    networks:
      - chat-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  ollama_data:
    driver: local

networks:
  chat-network:
    driver: bridge
